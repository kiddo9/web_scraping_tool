# ğŸ”¸ Web Scraping Tool

## ğŸ“Œ Overview

This is a **Web Scraping Tool** designed to extract structured data from websites at multiple levels of detail. It features both a **backend server** for processing and a **frontend interface** for user interaction.

---

## ğŸ› ï¸ Technologies Used

- **Frontend**

  - HTML5
  - CSS3
  - JavaScript

- **Backend**

  - Node.js
  - TypeScript
  - Express.js
  - Cheerio

---

## âœ¨ Features

- âœ… Interactive UI to submit URLs or search queries.
- âœ… Displays extracted data in a readable format.
- âœ… Accepts:

  - a **search query** (e.g., company name)
  - a **single URL**
  - multiple **seed URLs**

- âœ… Stores data in both **JSON** and **CSV** formats.
- âœ… Input validation to prevent invalid requests.
- âœ… Robust error handling to manage failed requests or invalid pages.

---

## ğŸ§  Extraction Levels

This tool offers **three levels of data insight**:

### ğŸ”¹ Level 1: Basic Info

- Company name
- Website URL
- Contact information (emails, phone numbers)

### ğŸ”¸ Level 2: Enhanced Details

- Social media profile links
- Location (physical or regional)
- Company description
- Products or services
- Market sector or industry

### ğŸ”¸ Level 3: Comprehensive Insight

- Tech stack used
- Market positioning
- Estimated company size

> **Design Decision**: The levels were introduced to modularize data extractionâ€”useful for scaling the scraper or applying resource-intensive extraction conditionally.

---

## âš™ï¸ Setup Instructions

### ğŸ“‹ Prerequisites

Ensure the following tools are installed:

- [Visual Studio Code](https://code.visualstudio.com/)
- [Node.js](https://nodejs.org/)
- Live Server extension (optional, for frontend testing)

### ğŸ—’ï¸ Steps to Set Up

1. **Clone the repository**:

   ```bash
   git clone https://github.com/kiddo9/web_scraping_tool.git
   ```

2. **Open the folder in VSCode**:

   ```bash
   code web_scraping_tool
   ```

3. **Navigate to the server directory**:

   ```bash
   cd server
   ```

4. **Install dependencies**:

   ```bash
   npm install
   ```

5. **Create a `.env` file** in the `server` directory:

   ```
   PORT=3000
   ```

> âš ï¸ Make sure your `.env` file is created in the **`server/`** folder.

---

## ğŸš€ Execution Instructions

### ğŸ’» Run Backend

Inside the `server` directory, start the backend:

```bash
npm run dev
```

### ğŸŒ Run Frontend

Use the **Live Server extension** in VSCode to open and run the `index.html` file located in the `client` directory (or root if there is no folder separation).

---

## ğŸ§© Assumptions & Design Decisions

- **Modular Levels**: The three extraction levels allow customization and scalingâ€”more advanced scrapes only run when needed.
- **Cheerio** was chosen for its lightweight DOM traversal capabilities, suitable for static HTML scraping.
- **No Headless Browser**: This scraper assumes **no JavaScript-rendered pages**. Future versions could integrate Puppeteer/Playwright for dynamic content.
- **Flat Data Storage**: JSON and CSV formats are used for simple downstream integration with spreadsheets, data platforms, or analytics.
- **Frontend Simplicity**: A simple vanilla JS UI keeps dependencies low and enables quick testing.

---

## ğŸ“¬ Feedback & Contributions

Suggestions, issues, or feature requests? Feel free to open an issue or submit a pull request.
